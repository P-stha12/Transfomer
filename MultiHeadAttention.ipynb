{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9foYFjaw1mhV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "max_seqlen = 200\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "x = tf.random.normal((batch_size, max_seqlen, d_model))"
      ],
      "metadata": {
        "id": "baVu3jTL1sEP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask = None):\n",
        "  scaled_dotproduct = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) / math.sqrt(q.shape[-1])\n",
        "  attention_weights = tf.nn.softmax(scaled_dotproduct, axis = -1)\n",
        "  if mask is not None:\n",
        "    attention_weights += mask\n",
        "  values = tf.matmul(attention_weights, v)\n",
        "  return values, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.Model):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.qkv_dense = Dense(d_model*3)\n",
        "    self.out_dense = Dense(d_model)\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    batch_size, max_seqlen, d_model = inputs.shape\n",
        "    qkv = self.qkv_dense(inputs)\n",
        "    qkv = tf.reshape(qkv, (batch_size, max_seqlen, self.n_heads, 3*self.head_dim))\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q, k, v = tf.split(qkv, 3, axis = -1)\n",
        "    values, attention_weights = scaled_dot_product(q, k, v, mask)\n",
        "    values = tf.reshape(values, (batch_size, max_seqlen, self.n_heads*self.head_dim))\n",
        "    out = self.out_dense(values)\n",
        "    return out"
      ],
      "metadata": {
        "id": "2wTRl2r61wh4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(d_model, n_heads)\n",
        "mha(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIpJSzaj1y6g",
        "outputId": "bc72ec77-30bb-40b8-ffa2-7c2c940b1ca4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 200, 512), dtype=float32, numpy=\n",
              "array([[[ 0.01795023, -0.12091614,  0.09743282, ..., -0.03686188,\n",
              "         -0.09144451, -0.01387916],\n",
              "        [-0.01523148, -0.07521614,  0.09003463, ...,  0.00406478,\n",
              "         -0.07066263, -0.04080007],\n",
              "        [ 0.0294656 , -0.07237387,  0.04118561, ..., -0.00842303,\n",
              "         -0.06011126, -0.02426854],\n",
              "        ...,\n",
              "        [ 0.06071714,  0.0051684 , -0.04929529, ..., -0.01382769,\n",
              "          0.03571779, -0.05440907],\n",
              "        [ 0.08154583, -0.00788801, -0.05433886, ..., -0.01039406,\n",
              "          0.09396115,  0.04686199],\n",
              "        [ 0.10124335,  0.0557582 , -0.01287371, ..., -0.04106165,\n",
              "          0.04083134,  0.03867629]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}