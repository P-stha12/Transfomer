{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9foYFjaw1mhV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "max_seqlen = 200\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "x = tf.random.normal((batch_size, max_seqlen, d_model))"
      ],
      "metadata": {
        "id": "baVu3jTL1sEP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask = None):\n",
        "  scaled_dotproduct = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) / math.sqrt(q.shape[-1])\n",
        "  attention_weights = tf.nn.softmax(scaled_dotproduct, axis = -1)\n",
        "  if mask is not None:\n",
        "    attention_weights += mask\n",
        "  values = tf.matmul(attention_weights, v)\n",
        "  return values, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.Model):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.qkv_dense = Dense(d_model*3)\n",
        "    self.out_dense = Dense(d_model)\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    batch_size, max_seqlen, d_model = inputs.shape\n",
        "    qkv = self.qkv_dense(inputs)\n",
        "    qkv = tf.reshape(qkv, (batch_size, max_seqlen, self.n_heads, 3*self.head_dim))\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q, k, v = tf.split(qkv, 3, axis = -1)\n",
        "    values, attention_weights = scaled_dot_product(q, k, v, mask)\n",
        "    out = self.out_dense(values)\n",
        "    return out"
      ],
      "metadata": {
        "id": "2wTRl2r61wh4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(d_model, n_heads)\n",
        "mha(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIpJSzaj1y6g",
        "outputId": "f83da5da-6bb7-453e-d315-01575a4d127f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 8, 200, 512), dtype=float32, numpy=\n",
              "array([[[[-0.03157103,  0.00248095,  0.03175261, ...,  0.01633478,\n",
              "           0.03105789,  0.00479053],\n",
              "         [-0.00023916,  0.00502656,  0.0308854 , ...,  0.00684066,\n",
              "           0.0341939 ,  0.03702442],\n",
              "         [ 0.00504123, -0.01155712,  0.02114235, ...,  0.01511279,\n",
              "           0.00699577,  0.00282884],\n",
              "         ...,\n",
              "         [-0.01668626, -0.02183168,  0.04970204, ...,  0.0194302 ,\n",
              "           0.01919152, -0.01221314],\n",
              "         [-0.01511907, -0.00300023,  0.01551432, ...,  0.01366398,\n",
              "           0.03087438,  0.01827713],\n",
              "         [-0.03672407, -0.00609656,  0.03056387, ...,  0.01352291,\n",
              "           0.0137173 , -0.0319193 ]],\n",
              "\n",
              "        [[-0.02130445, -0.01039708,  0.01162839, ...,  0.00250054,\n",
              "          -0.01752175,  0.01471678],\n",
              "         [-0.0239347 , -0.01950839,  0.02158425, ...,  0.01301685,\n",
              "          -0.01967132,  0.02036212],\n",
              "         [-0.02280619, -0.01825266,  0.01244256, ...,  0.03562924,\n",
              "          -0.00111773,  0.00812988],\n",
              "         ...,\n",
              "         [-0.02027969, -0.00361791,  0.01762118, ..., -0.00037879,\n",
              "           0.00071644,  0.04387929],\n",
              "         [-0.02703828, -0.03395428,  0.03674047, ..., -0.00953705,\n",
              "          -0.01536664,  0.04474919],\n",
              "         [-0.02383078, -0.02234615,  0.03560903, ...,  0.02185779,\n",
              "          -0.03204522,  0.02155571]],\n",
              "\n",
              "        [[ 0.06991892,  0.00015216, -0.04093033, ..., -0.02497286,\n",
              "           0.01016376,  0.00211568],\n",
              "         [ 0.06842489, -0.00503517, -0.03277904, ...,  0.0195355 ,\n",
              "           0.00335302,  0.00635604],\n",
              "         [ 0.04881679,  0.01293122, -0.03139481, ..., -0.00553941,\n",
              "           0.0154691 , -0.00604373],\n",
              "         ...,\n",
              "         [ 0.04320145, -0.03674366, -0.03952005, ...,  0.01526624,\n",
              "          -0.00941973, -0.0049904 ],\n",
              "         [ 0.01647473,  0.01576868, -0.00961404, ...,  0.02409732,\n",
              "           0.05158864, -0.00708092],\n",
              "         [ 0.0513152 , -0.01000636, -0.02889513, ...,  0.02082625,\n",
              "          -0.00937567,  0.02627911]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[ 0.00464625,  0.00318633,  0.01398567, ...,  0.03429356,\n",
              "           0.0091385 , -0.01042825],\n",
              "         [-0.01954356,  0.02244404, -0.01590535, ...,  0.03161997,\n",
              "          -0.01542102, -0.02117014],\n",
              "         [ 0.01719423,  0.00660844, -0.01213326, ...,  0.02219841,\n",
              "          -0.01441973, -0.03633963],\n",
              "         ...,\n",
              "         [ 0.00161064,  0.00113406,  0.00107349, ...,  0.0273168 ,\n",
              "          -0.01725046, -0.02510588],\n",
              "         [-0.01298873,  0.02916579, -0.00114151, ...,  0.01568404,\n",
              "          -0.00193105, -0.01356436],\n",
              "         [-0.0042388 ,  0.0022278 ,  0.00986073, ...,  0.01679439,\n",
              "          -0.00972041, -0.01166832]],\n",
              "\n",
              "        [[-0.00051689, -0.04983504,  0.0184521 , ..., -0.02717832,\n",
              "           0.04485709,  0.00845358],\n",
              "         [ 0.03845131, -0.02458043,  0.00092537, ..., -0.06332165,\n",
              "           0.03189757,  0.03297998],\n",
              "         [ 0.03051659, -0.03151864,  0.01760517, ..., -0.06618796,\n",
              "           0.02867726,  0.0330701 ],\n",
              "         ...,\n",
              "         [ 0.03563428, -0.01758522, -0.01224423, ..., -0.03763178,\n",
              "           0.04847341,  0.04655411],\n",
              "         [ 0.05954459, -0.04511167,  0.00136578, ..., -0.08513571,\n",
              "           0.02758887,  0.0354131 ],\n",
              "         [ 0.01444995, -0.04093823,  0.04291352, ..., -0.04218066,\n",
              "           0.05375524,  0.02880816]],\n",
              "\n",
              "        [[ 0.01722478,  0.01848943, -0.0108143 , ..., -0.0032458 ,\n",
              "          -0.03154782,  0.02812566],\n",
              "         [ 0.02356883,  0.01879546, -0.01369912, ...,  0.00749873,\n",
              "          -0.00836312,  0.02607573],\n",
              "         [-0.01784988,  0.00927841,  0.03788312, ...,  0.02209356,\n",
              "          -0.03358081,  0.00871013],\n",
              "         ...,\n",
              "         [ 0.02170752,  0.01413494, -0.00851837, ...,  0.00403928,\n",
              "           0.0090254 ,  0.02683233],\n",
              "         [ 0.02435477,  0.01178606, -0.00800819, ..., -0.00679739,\n",
              "          -0.02502663,  0.0278847 ],\n",
              "         [ 0.00718336,  0.00927224,  0.00696941, ...,  0.01120414,\n",
              "          -0.0104191 ,  0.00055203]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}