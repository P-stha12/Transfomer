{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9foYFjaw1mhV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "max_seqlen = 200\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "x = tf.random.normal((batch_size, max_seqlen, d_model))"
      ],
      "metadata": {
        "id": "baVu3jTL1sEP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask = None):\n",
        "  scaled_dotproduct = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) / math.sqrt(q.shape[-1])\n",
        "  if mask is not None:\n",
        "    scaled_dotproduct += mask\n",
        "  attention_weights = tf.nn.softmax(scaled_dotproduct, axis = -1)\n",
        "  values = tf.matmul(attention_weights, v)\n",
        "  return values, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.qkv_dense = Dense(d_model*3)\n",
        "    self.out_dense = Dense(d_model)\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    batch_size, max_seqlen, d_model = inputs.shape\n",
        "    qkv = self.qkv_dense(inputs)\n",
        "    qkv = tf.reshape(qkv, (batch_size, max_seqlen, self.n_heads, 3*self.head_dim))\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q, k, v = tf.split(qkv, 3, axis = -1)\n",
        "    values, attention_weights = scaled_dot_product(q, k, v, mask)\n",
        "    values = tf.reshape(values, (batch_size, max_seqlen, self.n_heads*self.head_dim))\n",
        "    out = self.out_dense(values)\n",
        "    return out"
      ],
      "metadata": {
        "id": "2wTRl2r61wh4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mha = MultiHeadAttention(d_model, n_heads)\n",
        "mha(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIpJSzaj1y6g",
        "outputId": "42c24c75-9837-41e4-81ed-4b62c7fce609"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 200, 512), dtype=float32, numpy=\n",
              "array([[[ 0.02877445, -0.01475262,  0.10030845, ...,  0.00272664,\n",
              "          0.00338816, -0.02589648],\n",
              "        [-0.00837563,  0.06653754,  0.09807103, ..., -0.05678674,\n",
              "         -0.03421912,  0.01303486],\n",
              "        [-0.07058281,  0.0352192 ,  0.03819446, ..., -0.06132259,\n",
              "         -0.01706199,  0.03019537],\n",
              "        ...,\n",
              "        [ 0.00886711,  0.00980199, -0.03666598, ..., -0.0303993 ,\n",
              "         -0.03986033, -0.0179459 ],\n",
              "        [ 0.00486959, -0.08574107,  0.00299693, ..., -0.04498035,\n",
              "         -0.02993053,  0.01326033],\n",
              "        [-0.00330408, -0.02367062, -0.01247103, ..., -0.00755949,\n",
              "         -0.0645581 ,  0.03299144]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    }
  ]
}