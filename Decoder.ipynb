{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f1gFNnGCpj3d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LayerNormalization\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "max_seqlen = 200\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "n_blocks = 5\n",
        "hidden = 1024\n",
        "drop_prob = 0.1"
      ],
      "metadata": {
        "id": "YwsDu0lHqJ_2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.normal((batch_size, max_seqlen, d_model))"
      ],
      "metadata": {
        "id": "_cYDpXYQpvil"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask = None):\n",
        "  scaled_dotproduct = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) / math.sqrt(q.shape[-1])\n",
        "  if mask is not None:\n",
        "    scaled_dotproduct += mask\n",
        "  attention_weights = tf.nn.softmax(scaled_dotproduct, axis = -1)\n",
        "  values = tf.matmul(attention_weights, v)\n",
        "  return values, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.qkv_dense = Dense(d_model*3)\n",
        "    self.out_dense = Dense(d_model)\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    batch_size, max_seqlen, d_model = inputs.shape\n",
        "    qkv = self.qkv_dense(inputs)\n",
        "    qkv = tf.reshape(qkv, (batch_size, max_seqlen, self.n_heads, 3*self.head_dim))\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q, k, v = tf.split(qkv, 3, axis = -1)\n",
        "    values, attention_weights = scaled_dot_product(q, k, v, mask)\n",
        "    values = tf.reshape(values, (batch_size, max_seqlen, self.n_heads*self.head_dim))\n",
        "    out = self.out_dense(values)\n",
        "    return out"
      ],
      "metadata": {
        "id": "6iZ5m5H7s2CQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding:\n",
        "  def __init__(self, max_seqlen, d_model):\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def call(self):\n",
        "    even_indices = tf.range(0, self.d_model, 2, dtype=tf.float32)\n",
        "    den = tf.math.pow(10000, 2*even_indices/self.d_model)\n",
        "    pos = tf.reshape(tf.range(0, self.max_seqlen, 1, dtype=tf.float32), (self.max_seqlen, 1))\n",
        "    even_pos = tf.math.sin(pos/den)\n",
        "    odd_pos = tf.math.cos(pos/den)\n",
        "    pos = tf.stack([even_pos, odd_pos], axis=2)\n",
        "    pos = tf.reshape(pos, (self.max_seqlen, self.d_model))\n",
        "    return pos"
      ],
      "metadata": {
        "id": "VKN4zVgIyuvx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.layer1 = Dense(hidden, activation='relu')\n",
        "    self.layer2 = Dense(d_model)\n",
        "    self.dropout = Dropout(drop_prob)\n",
        "        \n",
        "  def call(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "SVrrUMg5BL_b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadCrossAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, n_heads):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = d_model // n_heads\n",
        "        self.kv_layer = Dense(2*d_model)\n",
        "        self.q_layer = Dense(d_model)\n",
        "        self.linear = Dense(d_model)\n",
        "        \n",
        "    def call(self, x, y, mask=None):\n",
        "        batch_size, max_seqlen, d_model = x.shape\n",
        "        kv = self.kv_layer(x)\n",
        "        q = self.q_layer(y)\n",
        "        kv = tf.reshape(kv, (batch_size, max_seqlen, self.n_heads, 2*self.head_dim))\n",
        "        q = tf.reshape(q, (batch_size, max_seqlen, self.n_heads, self.head_dim))\n",
        "        kv = tf.transpose(kv, perm=[0, 2, 1, 3])\n",
        "        q = tf.transpose(q, perm=[0, 2, 1, 3])\n",
        "        k, v = tf.split(kv, 2, axis = -1)\n",
        "        values, attention = scaled_dot_product(q, k, v, mask)\n",
        "        values = tf.reshape(values, (batch_size, max_seqlen, self.n_heads*self.head_dim))\n",
        "        out = self.linear(values)\n",
        "        return out\n",
        "    \n",
        "    \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, n_heads, drop_prob):\n",
        "        super().__init__()\n",
        "        self.selfmha = MultiHeadAttention(d_model, n_heads)\n",
        "        self.norm1 = LayerNormalization()\n",
        "        self.dropout1 = Dropout(drop_prob)\n",
        "        self.crossmha = MultiHeadCrossAttention(d_model, n_heads)\n",
        "        self.norm2 = LayerNormalization()\n",
        "        self.dropout2 = Dropout(drop_prob)\n",
        "        self.ffn = FeedForward(d_model, ffn_hidden, drop_prob)\n",
        "        self.norm3 = LayerNormalization()\n",
        "        self.dropout3 = Dropout(drop_prob)\n",
        "        \n",
        "    def call(self, x, y, decoder_mask):\n",
        "        _y = y\n",
        "        y = self.selfmha(x)\n",
        "        y = self.dropout1(y)\n",
        "        y = self.norm1(y + _y)\n",
        "        _y = y\n",
        "        y = self.crossmha(x, y, decoder_mask)\n",
        "        y = self.dropout2(y)\n",
        "        y = self.norm2(_y+y)\n",
        "        _y = y\n",
        "        y = self.ffn(y)\n",
        "        y = self.dropout3(y)\n",
        "        y = self.norm3(y+_y)\n",
        "        \n",
        "        return y\n",
        "    \n",
        "    \n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, ffn_hidden, n_heads, drop_prob, n_blocks):\n",
        "        super().__init__()\n",
        "        self.decoders = [DecoderLayer(d_model, ffn_hidden, n_heads, drop_prob)\\\n",
        "                                         for _ in range(n_blocks)]\n",
        "    def call(self, x, y, mask):\n",
        "        for decoder in self.decoders:\n",
        "          y = decoder(x, y, mask)\n",
        "        return y"
      ],
      "metadata": {
        "id": "N6vqOVWuDpoO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 512\n",
        "num_heads = 8\n",
        "drop_prob = 0.1\n",
        "batch_size = 30\n",
        "max_sequence_length = 200\n",
        "ffn_hidden = 2048\n",
        "num_layers = 5\n",
        "\n",
        "x = tf.random.normal( (batch_size, max_sequence_length, d_model) )\n",
        "y = tf.random.normal( (batch_size, max_sequence_length, d_model) )\n",
        "mask = tf.fill([max_sequence_length, max_sequence_length] , float('-inf'))\n",
        "mask = tf.experimental.numpy.triu(mask, k=1)\n",
        "decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers)\n",
        "out = decoder(x, y, mask)"
      ],
      "metadata": {
        "id": "i9-SZHGhELVi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tX29mUWsRTJR",
        "outputId": "65f5c3be-007f-4909-b8d1-d5cd7b7fe57a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30, 200, 512), dtype=float32, numpy=\n",
              "array([[[ 0.13932322,  0.13584025, -0.4594161 , ...,  0.24291672,\n",
              "         -0.5508995 ,  1.515657  ],\n",
              "        [ 0.33804357,  0.41118777, -0.9975303 , ..., -0.9210208 ,\n",
              "          0.29964545,  0.32628384],\n",
              "        [-0.70708025, -0.21153395, -0.5459612 , ...,  0.27436   ,\n",
              "         -0.51453066,  1.382289  ],\n",
              "        ...,\n",
              "        [ 0.60298365,  0.00809366,  0.40203333, ..., -1.1408042 ,\n",
              "         -1.3849393 ,  1.0232967 ],\n",
              "        [ 0.05200108, -0.4694514 , -3.3843205 , ..., -1.2156051 ,\n",
              "         -0.49520615,  0.03540848],\n",
              "        [ 0.23965476,  0.40160403, -1.7326179 , ..., -1.0629336 ,\n",
              "         -1.5942286 ,  0.01689267]],\n",
              "\n",
              "       [[ 0.75725156, -2.4038734 , -0.80469906, ..., -1.6158617 ,\n",
              "         -0.16448896,  1.3181194 ],\n",
              "        [-0.0853722 , -0.7652514 , -0.64315945, ...,  1.2671533 ,\n",
              "         -0.86594146,  0.204378  ],\n",
              "        [-2.1040978 ,  0.16972065,  0.1381669 , ...,  0.73474175,\n",
              "         -1.8635955 ,  0.03357337],\n",
              "        ...,\n",
              "        [ 0.07097343, -0.54166967, -0.26949778, ..., -1.1549976 ,\n",
              "          1.6499536 ,  1.374716  ],\n",
              "        [-1.1062454 ,  0.5689864 , -1.9332825 , ..., -0.40844432,\n",
              "         -0.6501994 ,  2.4068012 ],\n",
              "        [-0.62071306, -1.0859978 ,  0.351693  , ..., -1.0981277 ,\n",
              "         -0.05558244,  1.2725314 ]],\n",
              "\n",
              "       [[ 0.9763324 , -0.5985675 ,  0.4103408 , ..., -3.144935  ,\n",
              "          1.3883911 ,  0.05170178],\n",
              "        [-0.7450845 , -0.02853077,  0.78702015, ..., -2.263648  ,\n",
              "          0.82031727,  2.1813755 ],\n",
              "        [ 0.9157392 ,  0.34810838, -0.92651784, ..., -1.7126005 ,\n",
              "          0.7258565 ,  1.879234  ],\n",
              "        ...,\n",
              "        [-0.6688953 ,  1.4258648 , -0.22379841, ..., -1.7305417 ,\n",
              "          0.48433396, -0.47074398],\n",
              "        [ 0.93088853,  0.13691854, -1.3282564 , ..., -1.0705616 ,\n",
              "         -0.40255   ,  0.1073034 ],\n",
              "        [ 0.54775554, -1.0264213 , -1.1980422 , ..., -3.2480683 ,\n",
              "          0.17339492,  0.524144  ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-0.33766583, -1.1914557 , -0.36179802, ..., -0.3068242 ,\n",
              "         -0.21119829,  0.22326729],\n",
              "        [ 0.6723444 , -0.600077  , -0.91231436, ..., -0.35974312,\n",
              "         -0.20860691,  2.1379483 ],\n",
              "        [ 0.0448809 ,  0.19524398, -1.0750263 , ..., -1.302088  ,\n",
              "         -0.44460073,  0.2535747 ],\n",
              "        ...,\n",
              "        [-0.9662134 , -0.5636447 , -1.1952211 , ..., -1.5514059 ,\n",
              "          1.6785923 ,  2.7975845 ],\n",
              "        [ 0.96516025, -1.3917472 , -0.6538939 , ..., -1.7575332 ,\n",
              "         -1.0287352 ,  0.7099199 ],\n",
              "        [-0.5896137 ,  1.616592  , -1.0153415 , ..., -1.3975855 ,\n",
              "          0.09006939,  0.47500113]],\n",
              "\n",
              "       [[-0.7276947 ,  0.67514694, -0.3090237 , ..., -1.3381525 ,\n",
              "         -0.13332942, -0.8401483 ],\n",
              "        [-1.3546913 ,  0.8354931 , -0.59709096, ..., -0.49311692,\n",
              "         -0.942939  ,  1.7445287 ],\n",
              "        [ 0.74707496, -0.21680248, -2.703168  , ..., -0.10232352,\n",
              "         -1.2952883 ,  1.087185  ],\n",
              "        ...,\n",
              "        [-0.30111003, -0.55224806, -2.2314065 , ..., -1.2135674 ,\n",
              "         -0.3764275 ,  3.0836554 ],\n",
              "        [-0.19291069, -0.5074802 , -0.14743198, ..., -2.4934514 ,\n",
              "          0.02472009, -0.6905033 ],\n",
              "        [ 0.10371398, -0.01043808,  0.35661218, ..., -1.4784251 ,\n",
              "         -0.5656385 , -0.19327447]],\n",
              "\n",
              "       [[-0.6261765 , -1.490329  , -0.77397186, ..., -0.83329666,\n",
              "         -0.28512383, -0.00725883],\n",
              "        [-1.0710211 , -1.7098346 ,  0.24832845, ..., -2.8433807 ,\n",
              "         -1.0317106 ,  0.75700974],\n",
              "        [ 0.34716177,  0.0346411 , -0.551502  , ..., -2.1066833 ,\n",
              "          0.35283357, -0.05713177],\n",
              "        ...,\n",
              "        [ 0.84148514, -1.0790764 , -0.8091615 , ..., -1.5944846 ,\n",
              "          1.2946707 ,  2.2118108 ],\n",
              "        [-0.3687941 ,  2.0242178 , -0.8844601 , ..., -0.27737793,\n",
              "         -0.5114401 ,  1.9341611 ],\n",
              "        [-0.04063337, -1.2677724 , -0.8475529 , ..., -1.886625  ,\n",
              "          0.61249804,  0.43459556]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}