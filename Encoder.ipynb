{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fRf0lROSTZ3L"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LayerNormalization\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "max_seqlen = 200\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "n_blocks = 5\n",
        "hidden = 1024\n",
        "drop_prob = 0.1"
      ],
      "metadata": {
        "id": "1u_BbDdYTbk1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.normal((batch_size, max_seqlen, d_model))"
      ],
      "metadata": {
        "id": "tv8cChJzTdwV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask = None):\n",
        "  scaled_dotproduct = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) / math.sqrt(q.shape[-1])\n",
        "  if mask is not None:\n",
        "    scaled_dotproduct += mask\n",
        "  attention_weights = tf.nn.softmax(scaled_dotproduct, axis = -1)\n",
        "  values = tf.matmul(attention_weights, v)\n",
        "  return values, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.qkv_dense = Dense(d_model*3)\n",
        "    self.out_dense = Dense(d_model)\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    batch_size, max_seqlen, d_model = inputs.shape\n",
        "    qkv = self.qkv_dense(inputs)\n",
        "    qkv = tf.reshape(qkv, (batch_size, max_seqlen, self.n_heads, 3*self.head_dim))\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q, k, v = tf.split(qkv, 3, axis = -1)\n",
        "    values, attention_weights = scaled_dot_product(q, k, v, mask)\n",
        "    values = tf.reshape(values, (batch_size, max_seqlen, self.n_heads*self.head_dim))\n",
        "    out = self.out_dense(values)\n",
        "    return out"
      ],
      "metadata": {
        "id": "6rGs1hNBTgbW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding:\n",
        "  def __init__(self, max_seqlen, d_model):\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def call(self):\n",
        "    even_indices = tf.range(0, self.d_model, 2, dtype=tf.float32)\n",
        "    den = tf.math.pow(10000, 2*even_indices/self.d_model)\n",
        "    pos = tf.reshape(tf.range(0, self.max_seqlen, 1, dtype=tf.float32), (self.max_seqlen, 1))\n",
        "    even_pos = tf.math.sin(pos/den)\n",
        "    odd_pos = tf.math.cos(pos/den)\n",
        "    pos = tf.stack([even_pos, odd_pos], axis=2)\n",
        "    pos = tf.reshape(pos, (self.max_seqlen, self.d_model))\n",
        "    return pos"
      ],
      "metadata": {
        "id": "Go31t21MTklB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.layer1 = Dense(hidden, activation='relu')\n",
        "    self.layer2 = Dense(d_model)\n",
        "    self.dropout = Dropout(drop_prob)\n",
        "        \n",
        "  def call(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "W4rqsj-LTovT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, n_heads, hidden, drop_prob):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(d_model, n_heads)\n",
        "    self.norm1 = LayerNormalization()\n",
        "    self.dropout1 = Dropout(drop_prob)\n",
        "    self.ffn = FeedForward(d_model, hidden, drop_prob)\n",
        "    self.norm2 = LayerNormalization()\n",
        "    self.dropout2 = Dropout(drop_prob)\n",
        "        \n",
        "  def call(self, x):\n",
        "    res = x\n",
        "    x = self.attention(x, mask=None)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.norm1(x+res)\n",
        "    res = x\n",
        "    x = self.ffn(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.norm2(x + res)\n",
        "    return x"
      ],
      "metadata": {
        "id": "RhHR-jUeTsoB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, n_blocks, d_model, n_heads, hidden, drop_prob):\n",
        "    super().__init__()\n",
        "    self.n_blocks = n_blocks\n",
        "    self.encoders = Sequential([EncoderLayer(d_model, n_heads, hidden, drop_prob) \\\n",
        "                                  for _ in range(n_blocks)])\n",
        "        \n",
        "  def call(self, inputs):\n",
        "    x = self.encoders(inputs)\n",
        "    return x"
      ],
      "metadata": {
        "id": "1R4bGKfoTwLv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = Encoder(n_blocks, d_model, n_heads, hidden, drop_prob)\n",
        "en(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdjAk5ZkT7pn",
        "outputId": "5739659c-15f1-4b5e-8cb5-dd2b196cccbc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 200, 512), dtype=float32, numpy=\n",
              "array([[[ 0.8720597 ,  0.72465485,  1.0577979 , ...,  2.0518732 ,\n",
              "         -0.11343463, -0.4436577 ],\n",
              "        [ 1.4336817 , -0.54340327,  1.1054131 , ...,  0.2823553 ,\n",
              "         -0.9729718 , -0.12151792],\n",
              "        [ 0.0448431 ,  1.3235494 ,  1.6045357 , ...,  1.3869084 ,\n",
              "          0.45061648, -0.51091754],\n",
              "        ...,\n",
              "        [-0.63664484,  1.7898706 ,  1.0812165 , ..., -0.05531621,\n",
              "         -0.4358906 , -0.18574098],\n",
              "        [ 0.9070335 , -0.18310636,  2.0024514 , ..., -0.6505261 ,\n",
              "         -1.6465125 , -0.5812339 ],\n",
              "        [-1.4111273 ,  1.0861882 ,  1.5656976 , ...,  1.3039907 ,\n",
              "         -1.1497822 ,  0.87188476]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}