{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f1gFNnGCpj3d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, LayerNormalization\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "max_seqlen = 200\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "n_blocks = 5\n",
        "hidden = 1024\n",
        "drop_prob = 0.1"
      ],
      "metadata": {
        "id": "YwsDu0lHqJ_2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.random.normal((batch_size, max_seqlen, d_model))"
      ],
      "metadata": {
        "id": "_cYDpXYQpvil"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product(q, k, v, mask = None):\n",
        "  scaled_dotproduct = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) / math.sqrt(q.shape[-1])\n",
        "  attention_weights = tf.nn.softmax(scaled_dotproduct, axis = -1)\n",
        "  if mask is not None:\n",
        "    attention_weights += mask\n",
        "  values = tf.matmul(attention_weights, v)\n",
        "  return values, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.Model):\n",
        "  def __init__(self, d_model, n_heads):\n",
        "    super().__init__()\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = d_model // n_heads\n",
        "    self.qkv_dense = Dense(d_model*3)\n",
        "    self.out_dense = Dense(d_model)\n",
        "\n",
        "  def call(self, inputs, mask=None):\n",
        "    batch_size, max_seqlen, d_model = inputs.shape\n",
        "    qkv = self.qkv_dense(inputs)\n",
        "    qkv = tf.reshape(qkv, (batch_size, max_seqlen, self.n_heads, 3*self.head_dim))\n",
        "    qkv = tf.transpose(qkv, perm=[0, 2, 1, 3])\n",
        "    q, k, v = tf.split(qkv, 3, axis = -1)\n",
        "    values, attention_weights = scaled_dot_product(q, k, v, mask)\n",
        "    values = tf.reshape(values, (batch_size, max_seqlen, self.n_heads*self.head_dim))\n",
        "    out = self.out_dense(values)\n",
        "    return out"
      ],
      "metadata": {
        "id": "6iZ5m5H7s2CQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding:\n",
        "  def __init__(self, max_seqlen, d_model):\n",
        "    self.max_seqlen = max_seqlen\n",
        "    self.d_model = d_model\n",
        "\n",
        "  def call(self):\n",
        "    even_indices = tf.range(0, self.d_model, 2, dtype=tf.float32)\n",
        "    den = tf.math.pow(10000, 2*even_indices/self.d_model)\n",
        "    pos = tf.reshape(tf.range(0, self.max_seqlen, 1, dtype=tf.float32), (self.max_seqlen, 1))\n",
        "    even_pos = tf.math.sin(pos/den)\n",
        "    odd_pos = tf.math.cos(pos/den)\n",
        "    pos = tf.stack([even_pos, odd_pos], axis=2)\n",
        "    pos = tf.reshape(pos, (self.max_seqlen, self.d_model))\n",
        "    return pos"
      ],
      "metadata": {
        "id": "VKN4zVgIyuvx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(tf.keras.Model):\n",
        "  def __init__(self, d_model, hidden, drop_prob=0.1):\n",
        "    super().__init__()\n",
        "    self.layer1 = Dense(hidden, activation='relu')\n",
        "    self.layer2 = Dense(d_model)\n",
        "    self.dropout = Dropout(drop_prob)\n",
        "        \n",
        "  def call(self, x):\n",
        "    x = self.layer1(x)\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer2(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "SVrrUMg5BL_b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.Model):\n",
        "  def __init__(self, d_model, n_heads, hidden, drop_prob):\n",
        "    super().__init__()\n",
        "    self.attention = MultiHeadAttention(d_model, n_heads)\n",
        "    self.norm1 = LayerNormalization()\n",
        "    self.dropout1 = Dropout(drop_prob)\n",
        "    self.ffn = FeedForward(d_model, hidden, drop_prob)\n",
        "    self.norm2 = LayerNormalization()\n",
        "    self.dropout2 = Dropout(drop_prob)\n",
        "        \n",
        "  def call(self, x):\n",
        "    res = x\n",
        "    x = self.attention(x, mask=None)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.norm1(x+res)\n",
        "    res = x\n",
        "    x = self.ffn(x)\n",
        "    x = self.dropout2(x)\n",
        "    x = self.norm2(x + res)\n",
        "    return x"
      ],
      "metadata": {
        "id": "N6vqOVWuDpoO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, n_blocks, d_model, n_heads, hidden, drop_prob):\n",
        "        super().__init__()\n",
        "        self.n_blocks = n_blocks\n",
        "        self.encoders = Sequential([EncoderLayer(d_model, n_heads, hidden, drop_prob) \\\n",
        "                                     for _ in range(n_blocks)])\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.encoders(inputs)\n",
        "        return x"
      ],
      "metadata": {
        "id": "i9-SZHGhELVi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en = Encoder(n_blocks, d_model, n_heads, hidden, drop_prob)"
      ],
      "metadata": {
        "id": "nASGjOGlEV7v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gze1lj8AEt7q",
        "outputId": "adb0e085-3741-4c93-cfa6-130ee877c232"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 200, 512), dtype=float32, numpy=\n",
              "array([[[ 8.2368737e-01,  1.7308272e-01, -4.9270141e-01, ...,\n",
              "          1.3057206e+00, -1.0925290e-01,  2.8918761e-01],\n",
              "        [-4.2751050e-01, -3.4096795e-01, -4.3196020e-01, ...,\n",
              "          1.0392998e+00,  1.0989662e+00,  8.3607346e-01],\n",
              "        [ 4.4611350e-01,  5.9474868e-01, -3.6346868e-01, ...,\n",
              "          2.7086868e+00,  1.4857063e+00,  1.1411437e+00],\n",
              "        ...,\n",
              "        [ 9.0194619e-01,  1.2233398e+00, -7.3240137e-01, ...,\n",
              "         -3.7522590e-01,  4.2617995e-01, -2.2641669e-03],\n",
              "        [-3.9516670e-01,  7.2224128e-01, -1.2510946e+00, ...,\n",
              "          1.2202094e+00,  1.1120749e+00, -2.4744701e-01],\n",
              "        [ 2.4680458e-01,  4.2367247e-01, -4.4969967e-01, ...,\n",
              "          4.3569800e-01,  9.1172850e-01, -8.6346075e-02]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}